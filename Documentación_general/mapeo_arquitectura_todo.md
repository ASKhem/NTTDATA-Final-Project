# Gu√≠a de Mapeo: TODO vs Arquitectura del Proyecto
Bas√°ndome en la imagen de la arquitectura del proyecto y el archivo TODO.md, aqu√≠ tienes una gu√≠a detallada que mapea cada tarea del TODO con los componentes espec√≠ficos de la arquitectura:

## üìä Bloque 1: Infraestructura y Datos
### Tarea 3: Documentaci√≥n Inicial
Ubicaci√≥n en Arquitectura: Documentaci√≥n transversal
Carpeta del Proyecto: 0_BigData/Documentacion_BigData/

- Modelo de datos ‚Üí Define la estructura de datos que fluir√° por todo el pipeline
- Plan de pruebas ‚Üí Establece validaciones para cada etapa del ETL
### Tarea 4: Pipeline ETL con Spark
Ubicaci√≥n en Arquitectura:

- Ingesta de datos: Cloud Functions (activaci√≥n) ‚Üí Cloud Scheduler
- Procesamiento: Cloud Dataproc (Spark) en la "Pipeline de datos (ETL)"
- Almacenamiento: Cloud Storage (capas raw/silver)
Carpeta del Proyecto: 0_BigData/Codigo_ETL/ETL_produccion/

Flujo espec√≠fico en la arquitectura:

1.  **Lectura desde GCS**: El pipeline lee desde la capa `raw`.
2.  **Procesamiento Spark**: El cl√∫ster de `Cloud Dataproc` ejecuta la limpieza y transformaci√≥n.
3.  **Escritura en Capa Silver**: El resultado se guarda como ficheros Parquet en la capa `silver` de GCS.
4.  **Consumo posterior**:
    *   **BI**: Un proceso posterior (ej. otra Cloud Function, Dataflow) carga los datos de la capa `silver` a `BigQuery`.
    *   **ML/EDA**: Los notebooks o pipelines de Vertex AI leen directamente de la capa `silver` para realizar la uni√≥n y el feature engineering.
### Tarea 5: Calidad del Dato
Ubicaci√≥n en Arquitectura: Integrado en el pipeline de Spark (Cloud Dataproc)
Carpeta del Proyecto: 0_BigData/Scripts_Adicionales_BigData/

- Las validaciones se ejecutan dentro del procesamiento Spark
- Los reportes se almacenan en BigQuery para consulta
### Tarea 6: Dashboard Power BI
Ubicaci√≥n en Arquitectura:

- Fuente de datos: BigQuery (DWH)
- Consumidor externo: Power BI (aparece como "Analista Stakeholder" en fuentes externas)
Carpeta del Proyecto: 0_BigData/Dashboard_PBI/

## ü§ñ Bloque 2: Inteligencia Artificial
### Tarea 7: Feature Engineering Avanzado
Ubicaci√≥n en Arquitectura:

- Datos fuente: BigQuery ‚Üí Pipeline de ML/Data (Bajo Demanda)
- Procesamiento: Vertex AI Workbench (DATAPROC) - zona derecha
- Almacenamiento features: Vertex AI Feature Store
Carpeta del Proyecto: 1_InteligenciaArtificial/Notebook_IA/

### Tarea 8: Entrenamiento y Evaluaci√≥n
Ubicaci√≥n en Arquitectura:

- Entrenamiento: Vertex AI Workbench ‚Üí Vertex AI Training
- Modelos: Vertex AI Model Registry
- Evaluaci√≥n: Vertex AI Pipelines (flujo completo)
Carpetas del Proyecto:

- 1_InteligenciaArtificial/Scripts_IA/ (c√≥digo de entrenamiento)
- 1_InteligenciaArtificial/Modelo_Entrenado/ (artefactos del modelo)
## üìã Bloque 3: Entrega Final
### Tarea 9: Consolidaci√≥n de Entregables
Ubicaci√≥n en Arquitectura: Transversal - todos los componentes
Carpeta del Proyecto: Todas las carpetas del proyecto

### Tarea 10: Documentaci√≥n Final
Ubicaci√≥n en Arquitectura: Documentaci√≥n del sistema completo
Carpeta del Proyecto: 1_InteligenciaArtificial/Informe_IA/

### Tarea 11: Entrega y Exposici√≥n
Ubicaci√≥n en Arquitectura: Presentaci√≥n del sistema completo
Carpeta del Proyecto: Ra√≠z del proyecto + presentaci√≥n

## üîÑ Flujo de Trabajo Recomendado
1. Documentaci√≥n (Tarea 3) ‚Üí Establece las bases
2. ETL Pipeline (Tarea 4) ‚Üí Implementa el flujo principal de datos
3. Calidad de Datos (Tarea 5) ‚Üí Valida el pipeline
4. Dashboard (Tarea 6) ‚Üí Visualiza los datos procesados
5. Feature Engineering (Tarea 7) ‚Üí Prepara datos para ML
6. Modelos IA (Tarea 8) ‚Üí Desarrolla la inteligencia artificial
7. Consolidaci√≥n (Tareas 9-11) ‚Üí Finaliza y presenta
## üí° Consejos de Implementaci√≥n
- Paralelizaci√≥n: Las tareas 3, 4 y 7 pueden trabajarse en paralelo una vez completada la documentaci√≥n inicial
- Dependencias cr√≠ticas: Tarea 4 debe completarse antes de la 5 y 6; Tarea 7 antes de la 8
- Iteraci√≥n: El desarrollo de IA (tareas 7-8) puede requerir volver a ajustar el ETL (tarea 4)
